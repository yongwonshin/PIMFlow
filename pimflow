#!/usr/bin/python3
from __future__ import absolute_import

from pim.util import *
from pim.transform import *

import onnx
import torch
import argparse
import os
import csv

def parse_arguments():
  parser = argparse.ArgumentParser()
  parser.add_argument("-m", "--mode", help="mode", choices=["profile", "solve", "run" ,"stat"], required=True)
  parser.add_argument("-t", "--transform", help="graph transformation", choices=["split", "pipeline"])
  parser.add_argument("-n", "--network", help="target network", choices=MODEL_LIST, required=True)
  parser.add_argument("--gpu_only", action="store_true", help="execute only on GPU")
  parser.add_argument("--conv_only", action="store_true", help="execute only convolution layers")
  args = parser.parse_args()

  if args.mode == "profile" and (args.transform is None):
    parser.error("-m/--mode requires -t/--transform")

  return args

def make_model(network):
  model = get_torch_model(network)
  model.cuda()
  model.half()
  model.eval()

  x = get_random_input(network)
  x = x.half()

  # Export the model
  torch.onnx.export(model,               # model being run
                    x,                         # model input (or a tuple for multiple inputs)
                    f"{network}.onnx",   # where to save the model (can be a file or file-like object)
                    export_params=True,        # store the trained parameter weights inside the model file
                    opset_version=13,          # the ONNX version to export the model to
                    do_constant_folding=True,  # whether to execute constant folding for optimization
                    # training=TrainingMode.TRAINING,
                    input_names = ['input'],   # the model's input names
                    output_names = ['output']) # the model's output names
                    # dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes
                    #               'output' : {0 : 'batch_size'}})

  onnx_model = onnx.load(f"{network}.onnx")
  onnx.checker.check_model(onnx_model)

  # infer shapes & preprocess
  onnx_model = onnx.shape_inference.infer_shapes(onnx_model)
  preprocess(onnx_model)
  return onnx_model

def profile(args):
  if args.transform == "split":
    os.system(f"cd layerwise && ./profile.sh {args.network}")
  elif args.transform == "pipeline":
    os.system(f"cd pipeline && ./profile.sh {args.network}")

def solve(args):
  os.system(f"cd solve && ./solve.sh {args.network}")

def stat(args):
  os.system(f"cd solve && ./stat.sh {args.network}")

def transform(args):
  split = {}
  pipeline = []
  with open(f'./{args.network}/solve_{args.network}.csv', newline='') as csvfile:
    reader = csv.reader(csvfile, delimiter=',')
    for row in reader:
      if row[1] == "split":
        split[row[0]] = int(row[2])
      elif row[2] == "pipeline":
        pipeline.append({'nodes': row[:2], 'is_gpu_first': int(row[4] != 1)})
      elif row[3] == "pipeline":
        pipeline.append({'nodes': row[:3], 'is_gpu_first': False})
      else:
        raise Exception("Must NOT reach here!")

  if not os.path.exists(f"{args.network}.onnx"):
    onnx_model = make_model(args.network)
  else:
    onnx_model = onnx.load(f"{args.network}.onnx")
  onnx.checker.check_model(onnx_model)

  node_map = {}
  # TODO[ywshin]
  # onnx_model = InputSplit(-1, split, node_map=node_map).transform(onnx_model)
  # for kv in pipeline:
  #   onnx_model = Pipeline(node_map=node_map).transform(onnx_model, kv['nodes'], stage=2, is_gpu_first=kv['is_gpu_first'])
  onnx.save(onnx_model, f"{args.network}_transformed_opt.onnx")
  print(node_map) # TODO[ywshin]

def run(args):
  pass

if __name__ == '__main__':
  args = parse_arguments()
  make_model(args.network)
  if args.mode == "profile":
    profile(args)
  elif args.mode == "solve":
    solve(args)
  elif args.mode == "run":
    transform(args)
    run(args)
  elif args.mode == "stat":
    stat(args)
  else:
    raise Exception("Must NOT reach here!")
